version: '3.8'

services:
  model:
    image: vllm/vllm-openai:latest
    container_name: model_container
    runtime: nvidia
    volumes:
      - ./models:/models
    environment:
      - HUGGING_FACE_HUB_TOKEN=$HF_TOKEN # this is mandatory for downloading models
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0} # default device is 0
    ipc: host
    networks:
      - chatbot_network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    command: >
      --model ${LLM_NAME:-Qwen/Qwen2.5-0.5B-Instruct}
      --download-dir /models
      --gpu-memory-utilization 0.9
      --max-model-len 8192
      --host 0.0.0.0

  chatbot:
    build:
      context: .
      dockerfile: dockerfiles/chatbot/Dockerfile
    container_name: chatbot_container
    networks:
      - chatbot_network
    depends_on:
      model:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8001/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    command: python3 chatbot_backend.py --llm-name ${LLM_NAME:-Qwen/Qwen2.5-0.5B-Instruct}

  webapp:
    build:
      context: .
      dockerfile: dockerfiles/streamlit/Dockerfile
    container_name: webapp_container
    ports:
      - "${EXTERNAL_PORT:-37347}:8051" # default port is 37347
    networks:
      - chatbot_network
    depends_on:
      chatbot:
        condition: service_healthy
    command: streamlit run web_app.py --server.port 8051

networks:
  chatbot_network:
    driver: bridge