version: '3.8'

services:
  model:
    image: vllm/vllm-openai:latest
    container_name: model_container
    runtime: nvidia
    volumes:
      - ./models:/models
    environment:
      - HUGGING_FACE_HUB_TOKEN=$HF_TOKEN # this is mandatory for downloading models
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0} # default device is 0
    ipc: host
    networks:
      - chatbot_network
    command: >
      --model ${LLM_NAME:-meta-llama/Llama-3.1-8B-Instruct}
      --download-dir /models
      --gpu-memory-utilization 0.9
      --max-model-len 8192
      --host 0.0.0.0

  chatbot:
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile.chatbot
    container_name: chatbot_container
    networks:
      - chatbot_network
    depends_on:
      - model
    command: python3 chatbot_backend.py --llm-name ${LLM_NAME:-meta-llama/Llama-3.1-8B-Instruct}

  webapp:
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile.streamlit
    container_name: webapp_container
    ports:
      - "${EXTERNAL_PORT:-37347}:8051" # default port is 37347
    networks:
      - chatbot_network
    depends_on:
      - chatbot
    command: streamlit run web_app.py --server.port 8051

networks:
  chatbot_network:
    driver: bridge